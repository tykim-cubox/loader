{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/loader/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import kornia as K\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from set_loader import CustomDataset, CustomAlbDataset, CustomKorDataset\n",
    "\n",
    "import cv2\n",
    "\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/loader/lib/python3.9/site-packages/albumentations/augmentations/dropout/cutout.py:50: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "p = 1.0\n",
    "albumentations_transform = A.Compose([\n",
    "    A.RandomCrop(256, 256, p=p),\n",
    "    A.HorizontalFlip(p=p),\n",
    "    A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), p=p),\n",
    "    A.dropout.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, p=p),\n",
    "    A.pytorch.ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpeg4py + albumentation \n",
    "root_path = '/home/aiteam/tykim/dataset/afhq/train'\n",
    "custom_ds = CustomAlbDataset(root_path, loader_type='jpeg4py', transform=albumentations_transform)\n",
    "dataloader = DataLoader(custom_ds, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.15326189994812] sec\n",
      "[15.743764162063599] sec\n",
      "[32.75279474258423] sec\n",
      "[19.616486072540283] sec\n",
      "[18.629342555999756] sec\n",
      "[11.323187112808228] sec\n",
      "[15.001330375671387] sec\n",
      "[15.834532976150513] sec\n",
      "[12.909913539886475] sec\n",
      "17 s ± 2.19 s per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for image, label in dataloader:\n",
    "    image = image.cuda()\n",
    "    label = label.cuda()\n",
    "    pass\n",
    "jpeg4py_alb_time = time.time() - start_time\n",
    "simple_load_times.append(jpeg4py_alb_time)\n",
    "print(str(simple_load_times) + ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFCV - JPEG 100 % quality\n",
    "\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder, RandomResizedCropRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToDevice, ToTensor, ToTorchImage, NormalizeImage, RandomHorizontalFlip, Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beton_path = '/home/aiteam/tykim/scratch/data_loaders/afhq_io_test.beton'\n",
    "\n",
    "# Random resized crop\n",
    "decoder = RandomResizedCropRGBImageDecoder(output_size=(256, 256))# SimpleRGBImageDecoder()\n",
    "mean = [0.5023*255, 0.4599*255, 0.3993*255]\n",
    "std = [0.2553*255, 0.2457*255, 0.2503*255]\n",
    "\n",
    "\n",
    "# Data decoding and augmentation\n",
    "image_pipeline = [decoder,  RandomHorizontalFlip(flip_prob=1.0), Cutout(8, tuple(map(int, mean))),\n",
    "                  ToTensor(), ToTorchImage(), \n",
    "                  ToDevice('cuda:0', non_blocking=True), NormalizeImage(np.array(mean), np.array(std), np.float32)]\n",
    "                  \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice('cuda:0')]\n",
    "\n",
    "# Pipeline for each data field\n",
    "pipelines = {\n",
    "    'image': image_pipeline,\n",
    "    'label': label_pipeline\n",
    "}\n",
    "\n",
    "# Replaces PyTorch data loader (`torch.utils.data.Dataloader`)\n",
    "loader = Loader(beton_path, batch_size=128, num_workers=8,\n",
    "                order=OrderOption.QUASI_RANDOM, pipelines=pipelines, os_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.98087763786316] sec\n",
      "[3.21522855758667] sec\n",
      "[3.1909334659576416] sec\n",
      "[3.099224805831909] sec\n",
      "[3.1774439811706543] sec\n",
      "[3.286438465118408] sec\n",
      "[3.2160937786102295] sec\n",
      "[3.2460453510284424] sec\n",
      "[3.1289079189300537] sec\n",
      "3.95 s ± 1.07 s per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for batch_idx, data in enumerate(loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "ffcv_time = time.time() - start_time\n",
    "simple_load_times.append(ffcv_time)\n",
    "print(str(simple_load_times) + ' sec') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DALI \n",
    "from nvidia.dali.pipeline import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "root_path = '/home/aiteam/tykim/dataset/afhq/train'\n",
    "\n",
    "@pipeline_def(batch_size=128, num_threads=8, device_id=0)\n",
    "def get_dali_pipeline(data_dir, dali_cpu=False, crop=256):\n",
    "  dali_device = 'cpu' if dali_cpu else 'gpu'\n",
    "  decoder_device = 'cpu' if dali_cpu else 'mixed'\n",
    "  # w, h = int(crop * 120), int(crop * 400)\n",
    "  # imagebytes = w * h * 3 * 4\n",
    "  \n",
    "  img_files, labels = fn.readers.file(file_root=data_dir, random_shuffle=False, name=\"Reader\")\n",
    "  \n",
    "  # Load and Crop\n",
    "  # 이미지 사이즈 힌트\n",
    "  preallocate_width_hint = 512 if decoder_device == 'mixed' else 0\n",
    "  preallocate_height_hint = 512 if decoder_device == 'mixed' else 0\n",
    "  \n",
    "  # images = fn.decoders.image(img_files, device=\"mixed\")\n",
    "  # Decode and Random Crop\n",
    "  images = fn.decoders.image_random_crop(img_files, device=decoder_device, preallocate_width_hint=preallocate_width_hint,\n",
    "                                         preallocate_height_hint=preallocate_height_hint,\n",
    "                                         output_type=types.RGB, random_aspect_ratio=[0.8, 1.25], random_area=[0.1, 1.0])\n",
    "  \n",
    "  # Resize\n",
    "  images = fn.resize(images,device=dali_device,resize_x=crop, resize_y=crop,interp_type=types.INTERP_TRIANGULAR)\n",
    "  # Horizontal F\n",
    "  images = fn.flip(images, device=dali_device, horizontal=1)\n",
    "\n",
    "\n",
    "  # Cutout\n",
    "  axis_names=\"WH\"\n",
    "  nregions=8\n",
    "  ndims = len(axis_names)\n",
    "  args_shape=(ndims*nregions,)\n",
    "  random_anchor = fn.random.uniform(range=(0., 1.), shape=args_shape)\n",
    "  random_shape = fn.random.uniform(range=(20., 50), shape=args_shape)\n",
    "  fn.erase(images, device=dali_device, anchor=random_anchor, shape=random_shape,\n",
    "            axis_names=axis_names, normalized_anchor=True,\n",
    "            normalized_shape=False)\n",
    "  # Normalization\n",
    "  images = fn.crop_mirror_normalize(images, device=dali_device,\n",
    "                                    dtype=types.FLOAT,\n",
    "                                    mean = [0.5023*255, 0.4599*255, 0.3993*255],\n",
    "                                    std = [0.2553*255, 0.2457*255, 0.2503*255])\n",
    "  return images, labels.gpu()\n",
    "\n",
    "pipe = get_dali_pipeline(data_dir=root_path)\n",
    "pipe.build()\n",
    "\n",
    "dataloader = DALIGenericIterator(pipe, ['data', 'label'],reader_name='Reader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1839418411254883] sec\n",
      "[1.1245269775390625] sec\n",
      "[1.1185104846954346] sec\n",
      "[1.121791124343872] sec\n",
      "[1.1238594055175781] sec\n",
      "[1.1269352436065674] sec\n",
      "[1.1241281032562256] sec\n",
      "[1.1282238960266113] sec\n",
      "[1.1090571880340576] sec\n",
      "1.13 s ± 9.55 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for i, data in enumerate(dataloader):\n",
    "  x, y = data[0]['data'], data[0]['label']  \n",
    "\n",
    "dali_time = time.time() - start_time\n",
    "simple_load_times.append(dali_time)\n",
    "print(str(simple_load_times) + ' sec') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiteam/miniconda3/envs/loader/lib/python3.9/site-packages/albumentations/augmentations/dropout/cutout.py:50: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "p = 1.0\n",
    "albumentations_transform = A.Compose([\n",
    "    A.RandomCrop(16, 16, p=p),\n",
    "    A.HorizontalFlip(p=p),\n",
    "    A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), p=p),\n",
    "    A.dropout.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, p=p),\n",
    "    A.pytorch.ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/aiteam/tykim/dataset/CIFAR-10-images/train'\n",
    "custom_ds = CustomAlbDataset(root_path, loader_type='jpeg4py', transform=albumentations_transform)\n",
    "dataloader = DataLoader(custom_ds, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6176698207855225] sec\n",
      "[4.637690305709839] sec\n",
      "[4.58872389793396] sec\n",
      "[4.410037279129028] sec\n",
      "[4.6040894985198975] sec\n",
      "[4.494801759719849] sec\n",
      "[4.464649438858032] sec\n",
      "[4.90467381477356] sec\n",
      "[5.105956554412842] sec\n",
      "4.65 s ± 134 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for image, label in dataloader:\n",
    "    image = image.cuda()\n",
    "    label = label.cuda()\n",
    "    pass\n",
    "jpeg4py_alb_time = time.time() - start_time\n",
    "simple_load_times.append(jpeg4py_alb_time)\n",
    "print(str(simple_load_times) + ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DALI \n",
    "from nvidia.dali.pipeline import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
    "\n",
    "root_path = '/home/aiteam/tykim/dataset/CIFAR-10-images/train'\n",
    "\n",
    "@pipeline_def(batch_size=128, num_threads=8, device_id=0)\n",
    "def get_dali_pipeline(data_dir, dali_cpu=False, crop=16):\n",
    "  dali_device = 'cpu' if dali_cpu else 'gpu'\n",
    "  decoder_device = 'cpu' if dali_cpu else 'mixed'\n",
    "  # w, h = int(crop * 120), int(crop * 400)\n",
    "  # imagebytes = w * h * 3 * 4\n",
    "  \n",
    "  img_files, labels = fn.readers.file(file_root=data_dir, random_shuffle=False, name=\"Reader\")\n",
    "  \n",
    "  # Load and Crop\n",
    "  # 이미지 사이즈 힌트\n",
    "  preallocate_width_hint = 32 if decoder_device == 'mixed' else 0\n",
    "  preallocate_height_hint = 32 if decoder_device == 'mixed' else 0\n",
    "  \n",
    "  # images = fn.decoders.image(img_files, device=\"mixed\")\n",
    "  # Decode and Random Crop\n",
    "  images = fn.decoders.image_random_crop(img_files, device=decoder_device, preallocate_width_hint=preallocate_width_hint,\n",
    "                                         preallocate_height_hint=preallocate_height_hint,\n",
    "                                         output_type=types.RGB, random_aspect_ratio=[0.8, 1.25], random_area=[0.1, 1.0])\n",
    "  \n",
    "  # Resize\n",
    "  images = fn.resize(images,device=dali_device,resize_x=crop, resize_y=crop,interp_type=types.INTERP_TRIANGULAR)\n",
    "  # Horizontal F\n",
    "  images = fn.flip(images, device=dali_device, horizontal=1)\n",
    "\n",
    "\n",
    "  # Cutout\n",
    "  axis_names=\"WH\"\n",
    "  nregions=8\n",
    "  ndims = len(axis_names)\n",
    "  args_shape=(ndims*nregions,)\n",
    "  random_anchor = fn.random.uniform(range=(0., 1.), shape=args_shape)\n",
    "  random_shape = fn.random.uniform(range=(20., 50), shape=args_shape)\n",
    "  fn.erase(images, device=dali_device, anchor=random_anchor, shape=random_shape,\n",
    "            axis_names=axis_names, normalized_anchor=True,\n",
    "            normalized_shape=False)\n",
    "  # Normalization\n",
    "  images = fn.crop_mirror_normalize(images, device=dali_device,\n",
    "                                    dtype=types.FLOAT,\n",
    "                                    mean = [0.5023*255, 0.4599*255, 0.3993*255],\n",
    "                                    std = [0.2553*255, 0.2457*255, 0.2503*255])\n",
    "  return images, labels.gpu()\n",
    "\n",
    "pipe = get_dali_pipeline(data_dir=root_path)\n",
    "pipe.build()\n",
    "\n",
    "dataloader = DALIGenericIterator(pipe, ['data', 'label'],reader_name='Reader')# DALI \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9557044506072998] sec\n",
      "[0.8870923519134521] sec\n",
      "[0.8955788612365723] sec\n",
      "[0.9180748462677002] sec\n",
      "[0.8786141872406006] sec\n",
      "[0.8949985504150391] sec\n",
      "[0.8926410675048828] sec\n",
      "[0.8861455917358398] sec\n",
      "[0.8754441738128662] sec\n",
      "898 ms ± 11.5 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for i, data in enumerate(dataloader):\n",
    "  x, y = data[0]['data'], data[0]['label']  \n",
    "\n",
    "dali_time = time.time() - start_time\n",
    "simple_load_times.append(dali_time)\n",
    "print(str(simple_load_times) + ' sec') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFCV - JPEG 100 % quality\n",
    "\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder, RandomResizedCropRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import ToDevice, ToTensor, ToTorchImage, NormalizeImage, RandomHorizontalFlip, Cutout\n",
    "\n",
    "beton_path = '/home/aiteam/tykim/scratch/data_loaders/cifar10_io_test.beton'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random resized crop\n",
    "decoder = RandomResizedCropRGBImageDecoder(output_size=(16, 16))# SimpleRGBImageDecoder()\n",
    "mean = [0.5023*255, 0.4599*255, 0.3993*255]\n",
    "std = [0.2553*255, 0.2457*255, 0.2503*255]\n",
    "\n",
    "\n",
    "# Data decoding and augmentation\n",
    "image_pipeline = [decoder,  RandomHorizontalFlip(flip_prob=1.0), Cutout(8, tuple(map(int, mean))),\n",
    "                  ToTensor(), ToTorchImage(), \n",
    "                  ToDevice('cuda:0', non_blocking=True), NormalizeImage(np.array(mean), np.array(std), np.float32)]\n",
    "                  \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice('cuda:0')]\n",
    "\n",
    "# Pipeline for each data field\n",
    "pipelines = {\n",
    "    'image': image_pipeline,\n",
    "    'label': label_pipeline\n",
    "}\n",
    "\n",
    "# Replaces PyTorch data loader (`torch.utils.data.Dataloader`)\n",
    "loader = Loader(beton_path, batch_size=128, num_workers=8,\n",
    "                order=OrderOption.QUASI_RANDOM, pipelines=pipelines, os_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.137673854827881] sec\n",
      "[0.42787671089172363] sec\n",
      "[0.3739004135131836] sec\n",
      "[0.3023409843444824] sec\n",
      "[0.31604552268981934] sec\n",
      "[0.31195735931396484] sec\n",
      "[0.419175386428833] sec\n",
      "[0.34900712966918945] sec\n",
      "[0.3009357452392578] sec\n",
      "The slowest run took 8.53 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1.1 s ± 1.09 s per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for batch_idx, data in enumerate(loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "ffcv_time = time.time() - start_time\n",
    "simple_load_times.append(ffcv_time)\n",
    "print(str(simple_load_times) + ' sec') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('loader')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a254999b829cf7a75923305dbce36972a67b91fdc16edd342b076b25e04d6382"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
